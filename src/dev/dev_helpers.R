owrite_tracker = function(network, domain, trck){
    tracker_path = glue::glue('data/{n}/{d}/data_tracker.json',
        n=network, d=domain)
    if(! missing(trck)) held_data = trck
    jsonlite::write_json(held_data, tracker_path)
}

export_all_local_helpers <- function(outfile){

    #needs work (run body interactively and it'll work)

    require(tidyverse)
    require(glue)

    sw = suppressWarnings

    try(setwd('~/git/macrosheds/data_acquisition'), silent=TRUE) #mike
    try(setwd('~/desktop/macrosheds/data_acquisition'), silent=TRUE) #spencer

    network_domain <- site_data %>%
        filter(as.logical(in_workflow)) %>%
        select(network, domain) %>%
        distinct() %>%
        arrange(network, domain)

    get_all_local_helpers <- function(network=domain, domain){

        #source_decoratees reads in decorator functions (tinsel package).
        #because it can only read them into the current environment, all files
        #sourced by this function are exported locally, then exported globally

        location1 = glue('src/{n}/network_helpers.R', n=network)
        if(file.exists(location1)){
            sw(source(location1, local=TRUE))
            # sw(source_decoratees(location1))
        }

        location2 = glue('src/{n}/{d}/domain_helpers.R', n=network, d=domain)
        if(file.exists(location2)){
            sw(source(location2, local=TRUE))
            # sw(source_decoratees(location2))
        }

        location3 = glue('src/{n}/processing_kernels.R', n=network)
        if(file.exists(location3)){
            sw(source(location3, local=TRUE))
            # sw(source_decoratees(location3))
        }

        location4 = glue('src/{n}/{d}/processing_kernels.R', n=network, d=domain)
        if(file.exists(location4)){
            sw(source(location4, local=TRUE))
            # sw(source_decoratees(location4))
        }

        rm(location1, location2, location3, location4)

        export_to_global(from_env=environment(),
            exclude=c('network', 'domain', 'thisenv'))

        return()
    }

    export_to_global <- function(from_env, exclude=NULL){

        #exclude is a character vector of names not to export.
        #unmatched names will be ignored.

        #vars could also be passed individually and handled by ...
        # vars = list(...)
        # varnames = all.vars(match.call())

        varnames = ls(name=from_env)
        varnames = varnames[! varnames %in% exclude]
        vars = mget(varnames, envir=from_env)

        for(i in 1:length(varnames)){
            assign(varnames[i], vars[[i]], .GlobalEnv)
        }

        return()
    }

    for(dmnrow in 1:nrow(network_domain)){

        network = network_domain$network[dmnrow]
        domain = network_domain$domain[dmnrow]

        get_all_local_helpers(network=network, domain=domain)
    }

    rm(ms_pasta_domain_refmap, export_all_local_helpers, dmnrow,
        domain, network, export_to_global, get_all_local_helpers,
        outfile, sw, envir=.GlobalEnv)

    write.csv(data.frame(funcname=ls()), outfile)

    return()
}

export_all_global_helpers <- function(outfile){

    #needs work (run body interactively and it'll work)

    try(setwd('~/git/macrosheds/data_acquisition'), silent=TRUE) #mike
    try(setwd('~/desktop/macrosheds/data_acquisition'), silent=TRUE) #spencer

    source('src/function_aliases.R')

    thisenv = environment()
    aliases = ls(envir=thisenv)

    source('src/global_helpers.R')

    rm(email_err_msgs, err_cnt, unique_errors, unique_exceptions, flagmap,
        envir=.GlobalEnv)
    rm(list=aliases, envir=.GlobalEnv)

    write.csv(data.frame(funcname=ls()), outfile)

    return()
}

compare_interp_methods <- function(){
    m1 <- reconstitute_raster(precip_idw, dem_wbj)
    m2 <- reconstitute_raster(precip_from_elev, dem_wbj)
    m3 <- reconstitute_raster(precip_interp, dem_wbj)
    par(mfrow=c(2, 3))
    # plot(idw_mask2, main='interpolation')
    plot(m0, main='idw') #generated by interpolate, see test kernel
    plot(m1, main='id^2w')
    plot(1, 1, type='n', axes=FALSE, xlab='', ylab='')
    plot(dem_wbj, main='dem')
    plot(m2, main='elev only')
    plot(m3, main='idw^2 and elev')
}

invalidate_tracked_data <- function(network, domain, level, prodname = NULL){

    #level is one of 'munge' or 'derive'. that level will be reset.
    #   not currently set up to invalidate level='retrieve'.
    #prodname is optional. if supplied, only that product will be invalidated.
    #   prodname_ms is determined automatically.

    if(! level %in% c('munge', 'derive')){
        stop('level must be either "munge" or "derive"')
    }

    tracker <- get_data_tracker(network = network,
                                domain = domain)

    if(! is.null(prodname)){

        prodnames_ms <- get_derive_ingredient(network, domain, prodname,
                              ignore_derprod = level == 'munge',
                              ignore_derprod900 = TRUE,
                              accept_multiple = TRUE)

        if(level == 'derive'){
            prodnames_ms <- prodnames_ms[grepl('__ms[0-9]{3}$', prodnames_ms)]
        }

        if(length(prodnames_ms) == 0){
            print(glue('Nothing to invalidate'))
            return(tracker)
        }

        message(glue('TEN SECONDS TO ABORT (Esc)...\ninvalidating {lvl} tracker(s):\n{prds}\n',
                     lvl = toupper(level),
                     prds = paste(prodnames_ms, collapse = '\n')))
        Sys.sleep(10)

        for(i in seq_along(prodnames_ms)){

            prodname_ms <- prodnames_ms[i]

            sublist <- tracker[[prodname_ms]]

            subl_updt <- recursive_tracker_update(l = sublist,
                                                  elem_name = level,
                                                  new_val = list(status = 'pending',
                                                                 mtime = '1500-01-01'))
            tracker[[prodname_ms]] <- subl_updt
        }

        return(tracker)

    } else {

        tracker <- recursive_tracker_update(l = tracker,
                                            elem_name = level,
                                            new_val = list(status = 'pending',
                                                           mtime = '1500-01-01'))
        return(tracker)
    }

    message('Tracker(s) invalidated.')
}

assign_typical_test_variables <- function(){

    try(assign('tracker', held_data, envir = .GlobalEnv))
    try(assign('i', 1, envir = .GlobalEnv))
    try(assign('j', 1, envir = .GlobalEnv))
    try(assign('k', 1, envir = .GlobalEnv))
    try(assign('sets', new_sets, envir = .GlobalEnv))
    try(assign('s', sets[i, ], envir = .GlobalEnv))
    try(assign('set_details', s, envir = .GlobalEnv))

    return()
}

compare_efficiency <- function(f, g, stepstart=10, stepstop=1e5, outfile){

    #f and g are expressions to be compared. they must both contain a variable
    #   `s` that will take a value of `step` for each iteration.
    #stepstart and stepstop are powers of 10. step is a sequence from stepstart
    #   to stepstop, increasing by powers of 10.

    #ratio easier to grasp if you make f the slower function (then it can be read
    #   as "f is this much slower than g")

    if(stepstart >= stepstop){
        stop('stepstart must be less than stepstop')
    }

    if(length(stepstart) != 1 || length(stepstop) != 1){
        stop('both stepstart and stepstop must be integers')
    }

    if(log10(stepstart) %% 1 != 0 || log10(stepstop) %% 1 != 0){
        stop('both stepstart and stepstop must be powers of 10')
    }

    step = 10 ^ seq(log10(stepstart), log10(stepstop))

    ftimes = gtimes = ratios = rep(NA, length = length(step))
    cnt = 1
    for(s in step){

        tt = Sys.time()
        eval(f)
        tt1 = difftime(Sys.time(), tt, units = 'sec')

        tt = Sys.time()
        eval(g)
        tt2 = difftime(Sys.time(), tt, units = 'sec')

        ftimes[cnt] = as.numeric(tt1)
        gtimes[cnt] = as.numeric(tt2)
        ratios[cnt] = as.numeric(tt1)/as.numeric(tt2)

        cnt = cnt + 1

        # print(paste0('err is ',
        #              round(as.numeric(tt2)/as.numeric(tt1), 2),
        #              'X slower'))
    }

    logstep = log(step)
    plotseq = seq(logstep[1], logstep[length(logstep)], 0.1)
    if(! missing(outfile)){
        mainlab = str_match(outfile, '.*/(.+)?\\.png$')[, 2]
    } else {
        mainlab = ''
    }

    # mf = lm(ftimes ~ logstep + I(logstep^2))
    # mg = lm(log(gtimes) ~ logstep)
    # mr = lm(ratios ~ logstep + I(logstep^2))
    # # p = predict(m, data.frame(x=plotseq))
    # # lines(plotseq, p)

    if(! missing(outfile)){
        png(width=5, height=5, units='in', res=300, filename=outfile,
            type='cairo')
    }

    par(mar=c(4, 4, 3, 4), oma=c(0, 0, 0, 0))
    ylims = range(c(ftimes, gtimes), na.rm=TRUE)
    # ylims = range(c(fitted(mf), fitted(mg)), na.rm=TRUE)
    plot(logstep, ftimes, col='red', ylim=ylims, type='l',
    # plot(logstep, fitted(mf), col='red', ylim=ylims, type='l',
         xlab = 'length', ylab = 'sec', xaxt='n', main=mainlab)
    options(scipen=-1000)
    axis(1, at=logstep, labels=step, las=3)
    options(scipen=0)
    lines(logstep, gtimes, col='blue')
    # lines(logstep, fitted(mg), col='blue')
    par(new = TRUE)
    plot(logstep, ratios, col='gray', type='l', yaxt='n', xaxt='n', xlab='',
    # plot(logstep, fitted(mr), col='gray', type='l', yaxt='n', xaxt='n', xlab='',
         ylab='', lty=3)
    axis(4)
    mtext('f-g ratio', 4, line=2.5, col='gray40')

    legend('top', legend=c('f', 'g', 'f:g'), col=c('red', 'blue', 'gray'),
           lty=c(1, 1, 3), bty='n')

    if(! missing(outfile)){
        dev.off()
    }

    return()
}

manufacture_uncert_msdf <- function(df, errval = 0.1){

    #colnames <- colnames(df)
    #noncan_colnames <- colnames[! colnames %in% ms_canonicals]

    #for(n in noncan_colnames){
    #    errors::errors(df[[n]]) <- errval
    #}
    errors(df$val) <- errval

    return(df)
}

testtb <- tibble(site_name = c('a','a','b','b'),
                 datetime = as.POSIXct(1:4, origin='2000-01-01'),
                 pH__val = 1:4, pHCODE = c('y','y','y','n'),
                 alk__val = 1:4, alkCODE = c('n','y','y','y'),
                 TYPE = c('g','b','g','b'))

delineate_watershed_test2 <- function(temp_dir = tmp,
                                      site_location_file = point_f,
                                      flow_accum_file = flow_f,
                                      d8_pointer_file = d8_f,
                                      snap_method,
                                      snap_dist){

    #this test func depends on environment variables that only exist at a
    #   certain point within delineate_watershed_apriori. scan through till
    #   you find the commented call to delineate_watershed_test2. it doesn't
    #   require re-download of a DEM, so is faster than delineate_watershed_test1.

    #temp_dir is created by tempdir() inside delineate_watershed_apriori
    #   or delineate_watershed_from_specs. its variable name is always "tmp"
    #site_location_file, flow_accum_file, and d8_pointer_file are
    #   created in delineate_watershed_apriori and
    #   delineate_watershed_from_specs.
    #snap_method is one of "jenson" or "standard"
    #snap_dist is the snapping distance in meters

    #returns an object that can be viewed with mapview::mapview()

    require(whitebox)

    test_file_base <- glue('{tem}/test_{typ}_dist{dst}',
                           tem = temp_dir,
                           typ = snap_method,
                           dst = snap_dist)

    test_snap_f <- paste0(test_file_base,
                          '.shp')
    test_wb_f <- paste0(test_file_base,
                        '.tif')

    if(snap_method == 'standard'){
        args <- list('pour_pts' = site_location_file,
                     'flow_accum' = flow_accum_file,
                     'output' = test_snap_f,
                     'snap_dist' = snap_dist)
        desired_func <- 'wbt_snap_pour_points'
    } else if(snap_method == 'jenson'){
        args <- list('pour_pts' = site_location_file,
                     'streams' = flow_accum_file,
                     'output' = test_snap_f,
                     'snap_dist' = snap_dist)
        desired_func <- 'wbt_jenson_snap_pour_points'
    } else {
        stop('snap_method must be "standard" or "jenson"')
    }

    do.call(desired_func, args)

    whitebox::wbt_watershed(d8_pntr = d8_pointer_file,
                            pour_pts = test_snap_f,
                            output = test_wb_f)

    test_wb_sf <- raster::raster(test_wb_f) %>%
        raster::rasterToPolygons() %>%
        sf::st_as_sf()

    map_out <- mapview::mapview(test_wb_sf)

    return(map_out)
}


delineate_watershed_test1 <- function(lat, long, crs,
                                      buffer_radius, dem_resolution,
                                      snap_method, snap_dist){

    #lat: numeric representing latitude in decimal degrees
    #   (negative indicates southern hemisphere)
    #long: numeric representing longitude in decimal degrees
    #   (negative indicates west of prime meridian)
    #crs: numeric representing the coordinate reference system (e.g. WSG84)

    #returns the location of the candidate watershed boundary file

    require(whitebox)
    require(mapview)

    tmp <- tempdir()
    inspection_dir <- glue(tmp, '/INSPECT_THESE')
    dem_f <- glue(tmp, '/dem.tif')
    point_f <- glue(tmp, '/point.shp')
    d8_f <- glue(tmp, '/d8_pntr.tif')
    flow_f <- glue(tmp, '/flow.tif')

    dir.create(path = inspection_dir,
               showWarnings = FALSE)

    proj <- choose_projection(lat = lat,
                              long = long)

    site <- tibble(x = lat,
                   y = long) %>%
        sf::st_as_sf(coords = c("y", "x"),
                     crs = crs) %>%
        sf::st_transform(proj)

    site_buf <- sf::st_buffer(x = site,
                              dist = buffer_radius)
    dem <- elevatr::get_elev_raster(locations = site_buf,
                                    z = dem_resolution)

    raster::writeRaster(x = dem,
                        filename = dem_f,
                        overwrite = TRUE)

    sf::st_write(obj = site,
                 dsn = point_f,
                 delete_layer = TRUE,
                 quiet = TRUE)

    whitebox::wbt_fill_single_cell_pits(dem = dem_f,
                                        output = dem_f)

    whitebox::wbt_breach_depressions(dem = dem_f,
                                     output = dem_f,
                                     flat_increment = 0.01)

    whitebox::wbt_d8_pointer(dem = dem_f,
                             output = d8_f)

    whitebox::wbt_d8_flow_accumulation(input = dem_f,
                                       output = flow_f,
                                       out_type = 'catchment area')

    test_file_base <- glue('{tem}/test_{typ}_dist{dst}',
                           tem = tmp,
                           typ = snap_method,
                           dst = snap_dist)

    test_snap_f <- paste0(test_file_base,
                          '.shp')
    test_wb_f <- paste0(test_file_base,
                        '.tif')

    if(snap_method == 'standard'){
        args <- list('pour_pts' = point_f,
                     'flow_accum' = flow_f,
                     'output' = test_snap_f,
                     'snap_dist' = snap_dist)
        desired_func <- 'wbt_snap_pour_points'
    } else if(snap_method == 'jenson'){
        args <- list('pour_pts' = point_f,
                     'streams' = flow_f,
                     'output' = test_snap_f,
                     'snap_dist' = snap_dist)
        desired_func <- 'wbt_jenson_snap_pour_points'
    } else {
        stop('snap_method must be "standard" or "jenson"')
    }

    do.call(desired_func, args)

    whitebox::wbt_watershed(d8_pntr = d8_f,
                            pour_pts = test_snap_f,
                            output = test_wb_f)

    test_wb_sf <- raster::raster(test_wb_f) %>%
        raster::rasterToPolygons() %>%
        sf::st_as_sf()

    map_out <- mapview(test_wb_sf) + mapview(dem)

    return(map_out)
}
# delineate_watershed_test1(lat, long, crs, buffer_radius = 100,
#                           dem_resolution = 10, snap_method = 'standard',
#                           snap_dist = 150)

populate_kernel_env <- function(include_extension = FALSE){

    #use this to quickly populate the variables needed inside a munge kernel,
    #   namely network, domain, site_name, prodname_ms, and component. Just
    #   call this function, then go to the console and
    #   tab-complete your way through a path (such as
    #   'data/lter/hbef/raw/precipitation__13/sitename_NA/HBEF daily precip.csv')
    #   the function will assign the relevant parts of the path to their
    #   respective variable names, and then you can step through the kernel!

    #include_extension: logical. if component has a file extension, like
    #   "stream chemistry.csv", should that be included (TRUE) or left off (FALSE)?

    cat(paste0("Enter path, e.g.: 'data/lter/hbef/raw/precipitation__13/",
               "sitename_NA/HBEF daily precip.csv'\n"))
    path <- scan(what = 'character',
                 nmax = 1,
                 quiet = TRUE)

    rgx <- str_match(path,
                     'data/([^/]+)/([^/]+)/raw/([^/]+)/([^/]+)/([^/]+)')

    network <- rgx[, 2]
    domain <- rgx[, 3]
    prodname_ms <- rgx[, 4]
    site_name <- rgx[, 5]

    if(include_extension){
        component <- rgx[, 6]
    } else {
        component <- str_match(rgx[, 6],
                               '(.+)?\\.[a-zA-Z0-9]+')[, 2]
    }

    assign('network', network, envir = .GlobalEnv)
    assign('domain', domain, envir = .GlobalEnv)
    assign('prodname_ms', prodname_ms, envir = .GlobalEnv)
    assign('site_name', site_name, envir = .GlobalEnv)
    assign('component', component, envir = .GlobalEnv)

    msg <- glue('network: {n}\ndomain: {d}\nprodname_ms: {p}\nsite_name: {s}\n',
                'component: {cp}',
                n = network,
                d = domain,
                p = prodname_ms,
                s = site_name,
                cp = component)

    message(msg)
}

pre_idw_filter_for_testing <- function(x, precip_only, daterange = NULL,
                                       length_days = 30){

    if(! is.null(daterange)){
        daterange = as.POSIXct(daterange)
    } else {
        if('list' %in% class(x)){
            daterange = x %>%
                purrr::map(~range(.x$datetime)) %>%
                purrr::reduce(function(a, b){
                    c(max(a[1], b[1]), min(a[2], b[2]))
                })
        } else {
            daterange = range(x$datetime)
        }
    }

    daterange = c(daterange[1], daterange[1] +
                      length_days * 60 * 60 * 24)
    drop_these = c()

    if('list' %in% class(x)){
        if(! precip_only){
            x = lapply(
                x,
                function(z){
                    filter(z, datetime < daterange[2], datetime > daterange[1])
                })

            drop_these = which(sapply(x, function(z) is_empty(z[[1]])))
            if(length(drop_these)) x = x[-drop_these]
        } else {
            return(list(x = NULL, daterange = NULL))
        }
    } else {
        x = filter(x, datetime < daterange[2], datetime > daterange[1])
    }

    return(list(x = x, drop_these = drop_these, daterange = daterange))
}

paste_from_excel = function(prefix, sep='__'){

    #you could paste a bunch of prodcodes into this with the prefix 'precipitation'
    #and it would output a neat quoted string, ready to be copied

    z = scan(what = character())
    paste0("'",
           paste(paste(prefix,
                       z,
                       sep = sep),
                 collapse = "', '"),
           "'")
}

invalidate_all = function(){
    for(dmnrow in 1:nrow(network_domain)){

        network <- network_domain$network[dmnrow]
        domain <- network_domain$domain[dmnrow]
        print(network); print(domain)

        tr = invalidate_tracked_data(network = network,
                                            domain = domain,
                                            level = 'munge')

        if(domain == 'hbef') print(tr$discharge__1$w1)
        owrite_tracker(network, domain, tr)
        if(domain == 'hbef'){
            tr2 = get_data_tracker(network, domain)
            print(tr2$discharge__1$w1)
        }

        tr = invalidate_tracked_data(network = network,
                                            domain = domain,
                                            level = 'derive')
        owrite_tracker(network, domain, tr)
    }
}

drop_automated_entries <- function(path = '.'){

    #this drops rows with "automated entry" from all products.csv files
    #found below the specified path

    system(glue("find {p} -name 'products.csv' | ",
                "xargs sed -e '/automated entry/d' -i.TMPBAK",
                p = path))

    system(glue("find <<p>> -name '*.TMPBAK' -exec rm {} \\;",
                p = path,
                .open = '<<',
                .close = '>>'))
}

dy_examine <- function(d, shape = 'long', site, ...){

    #TODO: to make this show gaps properly, we could use the new populate_implicit_NAs function

    #for quickly plotting time series in order to zoom in on points, so you
    #don't have to make an xts object and all that crap

    #d is either a standard ms tibble, or a tibble/df with datetime, site_name,
    #   and var columns.
    #shape is either 'long' (standard ms tibble) or 'wide'
    #site is a single site name, present in d$site_name. site can be NA if there
    #   is no site_name column
    #... = variable names

    #usage example: dy_examine(d, 'long', 'upper_ipswich', IS_discharge, GN_NH4_N)
    #(nonstandard evaluation works for anything passed to ...)

    dots = match.call(expand.dots = FALSE)$...
    arg_names = vapply(dots, as.character, '')

    if(! is.na(site)) d = filter(d, site_name == !!site)

    if(shape == 'wide'){

        if(any(! arg_names %in% colnames(d))){
            v = colnames(select(d, -one_of('site_name')))
            stop(glue('no data to plot. available variables for {s} are: {vv}',
                      s = site,
                      vv = paste(v, collapse = ", ")))
        }

        d = d[, c('datetime', arg_names)]

    } else if(shape == 'long'){

        if(any(! arg_names %in% unique(d$var))){
            stop(glue('no data to plot. available variables for {s} are: {vv}',
                      s = site,
                      vv = paste(unique(d$var), collapse = ", ")))
        }

        d = select(d, datetime, var, val)
        d = d %>%
            filter(var %in% arg_names) %>%
            pivot_wider(names_from = 'var',
                        values_from = 'val')

    } else {
        stop('shape must be "wide" or "long"')
    }

    d = xts::xts(select(d, -datetime),
             order.by = d$datetime,
             tzone = lubridate::tz(d$datetime[1]))

    dygraphs::dygraph(d) %>%
        dygraphs::dyOptions(
            useDataTimezone = TRUE,
            retainDateWindow = TRUE,
            labelsKMB = TRUE,
            connectSeparatedPoints = FALSE,
            drawPoints = FALSE,
            strokeWidth = 1,
            fillAlpha = 0.4,
            colors = rainbow(length(arg_names)),
            drawGapEdgePoints = TRUE)

            # fillAlpha = 1,
            # colors = raincolors[1],
            # strokeWidth = 3,
            # plotter = hyetograph_js,
            # stackedGraph = TRUE,
            # fillGraph = TRUE,
}

list_all_product_dirs <- function(prodname){

    prodname_dirs <- list.dirs(path = 'data',
                               full.names = TRUE,
                               recursive = TRUE)

    prodname_dirs <- grep(pattern = paste0('derived/', prodname, '__'),
                          x = prodname_dirs,
                          value = TRUE)

    return(prodname_dirs)
}

load_entire_product <- function(prodname, .sort = FALSE, filter_vars){

    #WARNING: this could easily eat up 20 GB RAM for a product like discharge.
    #As the dataset grows, that number will increase

    #read and combine all files associated with a particular prodname
    #   (e.g. 'discharge' or 'stream_chemistry') across all networks and
    #   domains. Run the setup portion of acquisition_master
    #   (the part before the main loop) to load necessary packages and helper
    #   functions
    #.sort: logical. If TRUE, output will be sorted by site_name, var, datetime.
    #   this takes a few minutes.
    #filter_vars: character vector. for products like stream_chemistry that include
    #   multiple variables, this filters to just the ones specified (ignoring
    #   prefix)

    prodname_dirs <- list_all_product_dirs(prodname = prodname)

    d <- tibble()
    for(pd in prodname_dirs){

        network_domain <- str_match(string = pd,
                                    pattern = '^data/(.+?)/(.+?)/.+$')[, 2:3]

        d0 <- list.files(pd, full.names = TRUE) %>%
            purrr::map_dfr(read_feather)

        if(! missing(filter_vars)){
            d0 <- filter(d0,
                        drop_var_prefix(var) %in% filter_vars)
        }

        d <- d0 %>%
            mutate(val = errors::set_errors(val, val_err),
                   network = network_domain[1],
                   domain = network_domain[2]) %>%
            select(-val_err) %>%
            select(datetime, network, domain, site_name, var, val, ms_status,
                   ms_interp) %>%
            bind_rows(d)
    }

    if(.sort){
        d <- arrange(d,
                     site_name, var, datetime)
    }

    return(d)
}
